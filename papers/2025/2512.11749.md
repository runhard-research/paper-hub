# SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder

- **arXiv**: https://arxiv.org/abs/2512.11749
- **alphaXiv**: https://www.alphaxiv.org/abs/2512.11749
- **PDF**: https://arxiv.org/pdf/2512.11749.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2512.11749
- **Tags**:#visual foundation models #text-to-image synthesis #self-supervised learning
- **Added**: 2026-01-02

---

## One-liner
Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation.
- Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored.
- To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain.
- By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench.

---

## Notes
Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
