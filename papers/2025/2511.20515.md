# AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs

- **arXiv**: https://arxiv.org/abs/2511.20515
- **alphaXiv**: https://www.alphaxiv.org/abs/2511.20515
- **PDF**: https://arxiv.org/pdf/2511.20515.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2511.20515
- **Tags**:#image-text alignment #clip models #benchmark evaluation
- **Added**: 2026-01-02

---

## One-liner
Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations.
- We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models.
- Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance.

---

## Notes
Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
