# Ministral 3

- **arXiv**: https://arxiv.org/abs/2601.08584
- **alphaXiv**: https://www.alphaxiv.org/abs/2601.08584
- **PDF**: https://arxiv.org/pdf/2601.08584.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2601.08584
- **Tags**:
- **Added**: 2026-01-15

---

## One-liner
We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters.

---

## Why I care
- Why I read this paper

---

## Key Ideas
- We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters.
- In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique.

---

## Notes
We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.

---

## alphaXiv discussion memo
- Comments that caught my attention
- A question I have
