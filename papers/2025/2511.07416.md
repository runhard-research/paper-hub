# Robot Learning from a Physical World Model

- **arXiv**: https://arxiv.org/abs/2511.07416
- **alphaXiv**: https://www.alphaxiv.org/abs/2511.07416
- **PDF**: https://arxiv.org/pdf/2511.07416.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2511.07416
- **Tags**:#robot learning #video generation #physical world modeling
- **Added**: 2026-01-02

---

## One-liner
We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling.
- However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations.
- Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model.
- Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches.

---

## Notes
We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
