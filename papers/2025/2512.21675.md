# UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture

- **arXiv**: https://arxiv.org/abs/2512.21675
- **alphaXiv**: https://www.alphaxiv.org/abs/2512.21675
- **PDF**: https://arxiv.org/pdf/2512.21675.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2512.21675
- **Tags**:#multimodal learning #perceptual-level understanding #visual rating
- **Added**: 2026-01-02

---

## One-liner
Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning.
- In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture.
- We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding.
- UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation.

---

## Notes
Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
