# Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting

- **arXiv**: https://arxiv.org/abs/2512.20927
- **alphaXiv**: https://www.alphaxiv.org/abs/2512.20927
- **PDF**: https://arxiv.org/pdf/2512.20927.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2512.20927
- **Tags**:#open-vocabulary segmentation #3d gaussian splatting #quantile rendering
- **Added**: 2026-01-02

---

## One-liner
Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS).

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality.
- To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity.
- By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner.
- Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps.

---

## Notes
Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
