# Depth Anything 3: Recovering the Visual Space from Any Views

- **arXiv**: https://arxiv.org/abs/2511.10647
- **alphaXiv**: https://www.alphaxiv.org/abs/2511.10647
- **PDF**: https://arxiv.org/pdf/2511.10647.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2511.10647
- **Tags**:#depth estimation #visual geometry #transformer models
- **Added**: 2026-01-02

---

## One-liner
We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses.
- Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2).
- Moreover, it outperforms DA2 in monocular depth estimation.

---

## Notes
We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
