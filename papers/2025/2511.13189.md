# Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework

- **arXiv**: https://arxiv.org/abs/2511.13189
- **alphaXiv**: https://www.alphaxiv.org/abs/2511.13189
- **PDF**: https://arxiv.org/pdf/2511.13189.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2511.13189
- **Tags**:#extreme multi-label classification #foundation models #vision-enhanced learning
- **Added**: 2026-01-02

---

## One-liner
Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC).

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures.
- We demonstrate that both play a critical role in XMC separately and can be combined for improved performance.
- We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable.
- Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image.

---

## Notes
Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
