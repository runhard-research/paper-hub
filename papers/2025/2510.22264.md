# PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding

- **arXiv**: https://arxiv.org/abs/2510.22264
- **alphaXiv**: https://www.alphaxiv.org/abs/2510.22264
- **PDF**: https://arxiv.org/pdf/2510.22264.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2510.22264
- **Tags**:#patent retrieval #sentence embeddings #multi-task learning
- **Added**: 2026-01-02

---

## One-liner
Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples.
- External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs.
- 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.

---

## Notes
Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
