# Part-X-MLLM: Part-aware 3D Multimodal Large Language Model

- **arXiv**: https://arxiv.org/abs/2511.13647
- **alphaXiv**: https://www.alphaxiv.org/abs/2511.13647
- **PDF**: https://arxiv.org/pdf/2511.13647.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2511.13647
- **Tags**:#3d multimodal #large language model #part-based generation
- **Added**: 2026-01-02

---

## One-liner
We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar.
- By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend.
- Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface.

---

## Notes
We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
