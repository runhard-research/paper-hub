# COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence

- **arXiv**: https://arxiv.org/abs/2512.04563
- **alphaXiv**: https://www.alphaxiv.org/abs/2512.04563
- **PDF**: https://arxiv.org/pdf/2512.04563.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2512.04563
- **Tags**:#multimodal large language models #visual spatial reasoning #auxiliary modality generation
- **Added**: 2026-01-02

---

## One-liner
Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation.
- In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence.
- We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities.
- COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance.

---

## Notes
Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
