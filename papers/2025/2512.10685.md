# Sharp Monocular View Synthesis in Less Than a Second

- **arXiv**: https://arxiv.org/abs/2512.10685
- **alphaXiv**: https://www.alphaxiv.org/abs/2512.10685
- **PDF**: https://arxiv.org/pdf/2512.10685.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2512.10685
- **Tags**:#photorealistic view synthesis #3d gaussian representation #zero-shot generalization
- **Added**: 2026-01-02

---

## One-liner
We present SHARP, an approach to photorealistic view synthesis from a single image.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- We present SHARP, an approach to photorealistic view synthesis from a single image.
- Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene.
- The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views.
- The representation is metric, with absolute scale, supporting metric camera movements.

---

## Notes
We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
