# InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization

- **arXiv**: https://arxiv.org/abs/2511.14899
- **alphaXiv**: https://www.alphaxiv.org/abs/2511.14899
- **PDF**: https://arxiv.org/pdf/2511.14899.pdf
- **HuggingFace Papers**: https://huggingface.co/papers/2511.14899
- **Tags**:#multi-view image editing #diffusion models #cross-view consistency
- **Added**: 2026-01-02

---

## One-liner
We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints.

---

## Why I care
- なぜこの論文を読んだか

---

## Key Ideas
- Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits.
- We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency.
- Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.

---

## Notes
We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.

---

## alphaXiv discussion memo
- 気になったコメント
- 自分の質問
